import os
import csv
import logging
import threading
import time
from datetime import datetime
from django.core.management.base import BaseCommand
from django.conf import settings
from django.db import transaction, connection
from django.utils import timezone
from concurrent.futures import ThreadPoolExecutor
import queue

logger = logging.getLogger(__name__)

class Command(BaseCommand):
    help = "–®–≤–∏–¥–∫–∏–π —ñ–º–ø–æ—Ä—Ç —Å—É–¥–æ–≤–∏—Ö —Ä—ñ—à–µ–Ω—å –∑ CSV —Ñ–∞–π–ª—ñ–≤ (–±–∞–≥–∞—Ç–æ–ø–æ—Ç–æ—á–Ω–∏–π)"

    def add_arguments(self, parser):
        parser.add_argument(
            "--year", 
            type=int, 
            help="–†—ñ–∫ –¥–ª—è —ñ–º–ø–æ—Ä—Ç—É (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, 2024)",
            required=True
        )
        parser.add_argument(
            "--threads", 
            type=int, 
            default=8,  # –ó–±—ñ–ª—å—à–µ–Ω–æ –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
            help="–ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ—Ç–æ–∫—ñ–≤ –¥–ª—è –æ–±—Ä–æ–±–∫–∏"
        )
        parser.add_argument(
            "--batch-size", 
            type=int, 
            default=50000,  # –ó–±—ñ–ª—å—à–µ–Ω–æ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—ó —à–≤–∏–¥–∫–æ—Å—Ç—ñ
            help="–†–æ–∑–º—ñ—Ä –±–∞—Ç—á—É –¥–ª—è bulk_create"
        )
        parser.add_argument(
            "--chunk-size", 
            type=int, 
            default=200000,  # –ó–±—ñ–ª—å—à–µ–Ω–æ –¥–ª—è –º–µ–Ω—à–æ—ó –∫—ñ–ª—å–∫–æ—Å—Ç—ñ I/O –æ–ø–µ—Ä–∞—Ü—ñ–π
            help="–†–æ–∑–º—ñ—Ä —á–∞–Ω–∫–∞ –¥–ª—è —á–∏—Ç–∞–Ω–Ω—è CSV"
        )

    def handle(self, *args, **options):
        year = options["year"]
        threads = options["threads"]
        batch_size = options["batch_size"]
        chunk_size = options["chunk_size"]
        
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ —à–ª—è—Ö –¥–æ CSV —Ñ–∞–π–ª—É
        if year >= 2000:
            short_year = year - 2000
        else:
            short_year = year - 1900
        
        csv_filename = f"documents_{short_year:02d}.csv"
        csv_path = os.path.join(settings.BASE_DIR, "data", csv_filename)
        
        if not os.path.exists(csv_path):
            self.stdout.write(
                self.style.ERROR(f"–§–∞–π–ª {csv_filename} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó data/")
            )
            return
        
        self.stdout.write(f"üöÄ –®–í–ò–î–ö–ò–ô –Ü–ú–ü–û–†–¢ —Å—É–¥–æ–≤–∏—Ö —Ä—ñ—à–µ–Ω—å –∑–∞ {year} —Ä—ñ–∫")
        self.stdout.write(f"üìÅ –§–∞–π–ª: {csv_filename}")
        self.stdout.write(f"üîß –ü–æ—Ç–æ–∫—ñ–≤: {threads}, –ë–∞—Ç—á: {batch_size}, –ß–∞–Ω–∫: {chunk_size}")
        
        # –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –∑–∞–≥–∞–ª—å–Ω–æ—ó –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∑–∞–ø–∏—Å—ñ–≤
        total_records = self._count_csv_records(csv_path)
        self.stdout.write(f"üìä –í—Å—å–æ–≥–æ –∑–∞–ø–∏—Å—ñ–≤ —É CSV: {total_records:,}")
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–∞–±–ª–∏—Ü—ñ –∑ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è–º–∏
        table_name = self._create_optimized_table(year)
        
        # –ó–∞–ø—É—Å–∫ –±–∞–≥–∞—Ç–æ–ø–æ—Ç–æ—á–Ω–æ–≥–æ —ñ–º–ø–æ—Ä—Ç—É
        start_time = time.time()
        imported_count = self._parallel_import(
            csv_path, table_name, threads, batch_size, chunk_size, total_records
        )
        end_time = time.time()
        
        duration = end_time - start_time
        records_per_second = imported_count / duration if duration > 0 else 0
        
        self.stdout.write(
            self.style.SUCCESS(
                f"‚úÖ –Ü–º–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {duration:.1f} —Å–µ–∫!\n"
                f"üìà –û–±—Ä–æ–±–ª–µ–Ω–æ: {imported_count:,} –∑–∞–ø–∏—Å—ñ–≤\n"
                f"‚ö° –®–≤–∏–¥–∫—ñ—Å—Ç—å: {records_per_second:,.0f} –∑–∞–ø–∏—Å—ñ–≤/—Å–µ–∫"
            )
        )

    def _count_csv_records(self, csv_path):
        """–®–≤–∏–¥–∫–∏–π –ø—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∑–∞–ø–∏—Å—ñ–≤"""
        try:
            with open(csv_path, "rb") as file:
                lines = sum(1 for _ in file) - 1  # –ú—ñ–Ω—É—Å –∑–∞–≥–æ–ª–æ–≤–æ–∫
                return lines
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"–ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è CSV: {e}"))
            return 0

    def _create_optimized_table(self, year):
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ—ó —Ç–∞–±–ª–∏—Ü—ñ"""
        table_name = f"court_decisions_{year}"
        
        with connection.cursor() as cursor:
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —ñ—Å–Ω—É—î —Ç–∞–±–ª–∏—Ü—è
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_name = %s
                );
            """, [table_name])
            
            if cursor.fetchone()[0]:
                self.stdout.write(f"üìã –¢–∞–±–ª–∏—Ü—è {table_name} –≤–∂–µ —ñ—Å–Ω—É—î")
                return table_name
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–∞–±–ª–∏—Ü—é –ë–ï–ó —ñ–Ω–¥–µ–∫—Å—ñ–≤ –¥–ª—è —à–≤–∏–¥–∫–æ—ó –≤—Å—Ç–∞–≤–∫–∏
            create_table_sql = f"""
                CREATE UNLOGGED TABLE {table_name} (
                    id SERIAL PRIMARY KEY,
                    doc_id VARCHAR(50) NOT NULL,
                    court_code VARCHAR(20),
                    judgment_code VARCHAR(10),
                    justice_kind VARCHAR(10),
                    category_code VARCHAR(20),
                    cause_num VARCHAR(255),
                    adjudication_date TIMESTAMP,
                    receipt_date TIMESTAMP,
                    judge VARCHAR(500),
                    doc_url TEXT,
                    status VARCHAR(10),
                    date_publ TIMESTAMP,
                    court_name VARCHAR(500),
                    judgment_name VARCHAR(200),
                    justice_kind_name VARCHAR(200),
                    category_name VARCHAR(200),
                    resolution_text TEXT,
                    import_date TIMESTAMP DEFAULT NOW()
                );
            """
            
            cursor.execute(create_table_sql)
            
            # –ë–∞–∑–æ–≤—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è —à–≤–∏–¥–∫–æ—ó –≤—Å—Ç–∞–≤–∫–∏
            try:
                cursor.execute("SET synchronous_commit = off;")
            except:
                pass  # –Ü–≥–Ω–æ—Ä—É—î–º–æ –ø–æ–º–∏–ª–∫–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å
            
            self.stdout.write(f"üÜï –°—Ç–≤–æ—Ä–µ–Ω–æ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω—É —Ç–∞–±–ª–∏—Ü—é {table_name}")
        
        return table_name

    def _parallel_import(self, csv_path, table_name, threads, batch_size, chunk_size, total_records):
        """–ë–∞–≥–∞—Ç–æ–ø–æ—Ç–æ—á–Ω–∏–π —ñ–º–ø–æ—Ä—Ç"""
        
        # –ß–µ—Ä–≥–∞ –¥–ª—è —á–∞–Ω–∫—ñ–≤ –¥–∞–Ω–∏—Ö
        data_queue = queue.Queue(maxsize=threads * 2)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.imported_count = 0
        self.processed_chunks = 0
        self.lock = threading.Lock()
        
        # –ó–∞–ø—É—Å–∫ –º–æ–Ω—ñ—Ç–æ—Ä—É –ø—Ä–æ–≥—Ä–µ—Å—É
        stop_monitor = threading.Event()
        monitor_thread = threading.Thread(
            target=self._progress_monitor, 
            args=(stop_monitor, total_records)
        )
        monitor_thread.start()
        
        # –ó–∞–ø—É—Å–∫ –≤–æ—Ä–∫–µ—Ä—ñ–≤
        with ThreadPoolExecutor(max_workers=threads) as executor:
            # –ó–∞–ø—É—Å–∫–∞—î–º–æ –≤–æ—Ä–∫–µ—Ä–∏
            futures = []
            for i in range(threads):
                future = executor.submit(
                    self._worker_thread, 
                    data_queue, table_name, batch_size, i
                )
                futures.append(future)
            
            # –ß–∏—Ç–∞—î–º–æ —Ñ–∞–π–ª —á–∞–Ω–∫–∞–º–∏ —ñ –¥–æ–¥–∞—î–º–æ –¥–æ —á–µ—Ä–≥–∏
            self._read_csv_chunks(csv_path, chunk_size, data_queue)
            
            # –°–∏–≥–Ω–∞–ª—ñ–∑—É—î–º–æ –∫—ñ–Ω–µ—Ü—å –¥–∞–Ω–∏—Ö
            for _ in range(threads):
                data_queue.put(None)
            
            # –ß–µ–∫–∞—î–º–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è –≤—Å—ñ—Ö –≤–æ—Ä–∫–µ—Ä—ñ–≤
            for future in futures:
                future.result()
        
        # –ó—É–ø–∏–Ω—è—î–º–æ –º–æ–Ω—ñ—Ç–æ—Ä
        stop_monitor.set()
        monitor_thread.join()
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —ñ–Ω–¥–µ–∫—Å–∏ –ø—ñ—Å–ª—è —ñ–º–ø–æ—Ä—Ç—É
        self._create_indexes_after_import(table_name)
        
        return self.imported_count

    def _read_csv_chunks(self, csv_path, chunk_size, data_queue):
        """–ß–∏—Ç–∞–Ω–Ω—è CSV —á–∞–Ω–∫–∞–º–∏"""
        try:
            with open(csv_path, "r", encoding="utf-8") as file:
                reader = csv.DictReader(file, delimiter="\t")
                
                chunk = []
                for row in reader:
                    chunk.append(row)
                    
                    if len(chunk) >= chunk_size:
                        data_queue.put(chunk.copy())
                        chunk = []
                
                # –î–æ–¥–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π —á–∞–Ω–∫
                if chunk:
                    data_queue.put(chunk)
                    
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"–ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è CSV: {e}"))

    def _worker_thread(self, data_queue, table_name, batch_size, worker_id):
        """–í–æ—Ä–∫–µ—Ä –¥–ª—è –æ–±—Ä–æ–±–∫–∏ —á–∞–Ω–∫—ñ–≤"""
        while True:
            chunk = data_queue.get()
            if chunk is None:
                break
            
            try:
                processed = self._process_chunk(chunk, table_name, batch_size)
                
                with self.lock:
                    self.imported_count += processed
                    self.processed_chunks += 1
                    
            except Exception as e:
                logger.error(f"–ü–æ–º–∏–ª–∫–∞ –≤ –≤–æ—Ä–∫–µ—Ä—ñ {worker_id}: {e}")
            finally:
                data_queue.task_done()

    def _process_chunk(self, chunk, table_name, batch_size):
        """–û–±—Ä–æ–±–∫–∞ —á–∞–Ω–∫–∞ –¥–∞–Ω–∏—Ö"""
        processed_count = 0
        batch_data = []
        
        for row in chunk:
            processed_row = self._process_csv_row(row)
            batch_data.append(processed_row)
            
            if len(batch_data) >= batch_size:
                inserted = self._bulk_insert_batch(table_name, batch_data)
                processed_count += inserted
                batch_data = []
        
        # –û—Å—Ç–∞–Ω–Ω—ñ–π –±–∞—Ç—á
        if batch_data:
            inserted = self._bulk_insert_batch(table_name, batch_data)
            processed_count += inserted
        
        return processed_count

    def _bulk_insert_batch(self, table_name, batch_data):
        """–ú–∞—Å–æ–≤–∞ –≤—Å—Ç–∞–≤–∫–∞ –±–∞—Ç—á—É –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ COPY"""
        if not batch_data:
            return 0
        
        try:
            with connection.cursor() as cursor:
                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ PostgreSQL COPY –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—ó —à–≤–∏–¥–∫–æ—Å—Ç—ñ
                copy_sql = f"""
                    COPY {table_name} 
                    (doc_id, court_code, judgment_code, justice_kind, category_code, 
                     cause_num, adjudication_date, receipt_date, judge, doc_url, 
                     status, date_publ, court_name, judgment_name, justice_kind_name, 
                     category_name, resolution_text, import_date)
                    FROM STDIN WITH CSV
                """
                
                # –ü—ñ–¥–≥–æ—Ç–æ–≤–ª—é—î–º–æ –¥–∞–Ω—ñ –¥–ª—è COPY
                import io
                data_io = io.StringIO()
                for row_data in batch_data:
                    values = [
                        self._escape_csv_value(row_data["doc_id"]),
                        self._escape_csv_value(row_data["court_code"]),
                        self._escape_csv_value(row_data["judgment_code"]),
                        self._escape_csv_value(row_data["justice_kind"]),
                        self._escape_csv_value(row_data["category_code"]),
                        self._escape_csv_value(row_data["cause_num"]),
                        row_data["adjudication_date"].isoformat() if row_data["adjudication_date"] else "",
                        row_data["receipt_date"].isoformat() if row_data["receipt_date"] else "",
                        self._escape_csv_value(row_data["judge"]),
                        self._escape_csv_value(row_data["doc_url"]),
                        self._escape_csv_value(row_data["status"]),
                        row_data["date_publ"].isoformat() if row_data["date_publ"] else "",
                        self._escape_csv_value(row_data["court_name"]),
                        self._escape_csv_value(row_data["judgment_name"]),
                        self._escape_csv_value(row_data["justice_kind_name"]),
                        self._escape_csv_value(row_data["category_name"]),
                        self._escape_csv_value(row_data["resolution_text"]),
                        row_data["import_date"].isoformat()
                    ]
                    data_io.write(",".join(values) + "\n")
                
                data_io.seek(0)
                cursor.copy_expert(copy_sql, data_io)
                
                return len(batch_data)
                
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ bulk insert: {e}")
            return 0

    def _escape_csv_value(self, value):
        """–ï–∫—Ä–∞–Ω—É–≤–∞–Ω–Ω—è –∑–Ω–∞—á–µ–Ω—å –¥–ª—è CSV"""
        if value is None:
            return ""
        
        value_str = str(value).replace(""", """")  # –ï–∫—Ä–∞–Ω—É–≤–∞–Ω–Ω—è –ª–∞–ø–æ–∫
        if "," in value_str or "\n" in value_str or """ in str(value):
            return f""{value_str}""
        
        return value_str

    def _process_csv_row(self, row):
        """–û–±—Ä–æ–±–∫–∞ –æ–¥–Ω–æ–≥–æ —Ä—è–¥–∫–∞ CSV"""
        def parse_date(date_str):
            if not date_str or date_str.strip() == "":
                return None
            
            try:
                # –§–æ—Ä–º–∞—Ç: "2024-01-01 00:00:00+02"
                if "+" in date_str or "-" in date_str[-6:]:
                    date_part = date_str.split("+")[0].split(" ")[0] if "+" in date_str else date_str.split("-")[0]
                    return datetime.strptime(date_part, "%Y-%m-%d")
                elif len(date_str) >= 10:
                    return datetime.strptime(date_str[:10], "%Y-%m-%d")
                
                return None
            except:
                return None

        return {
            "doc_id": row.get("doc_id", "").strip()[:50],
            "court_code": row.get("court_code", "").strip()[:20],
            "judgment_code": row.get("judgment_code", "").strip()[:10],
            "justice_kind": row.get("justice_kind", "").strip()[:10],
            "category_code": row.get("category_code", "").strip()[:20],
            "cause_num": row.get("cause_num", "").strip()[:255],
            "adjudication_date": parse_date(row.get("adjudication_date", "")),
            "receipt_date": parse_date(row.get("receipt_date", "")),
            "judge": row.get("judge", "").strip()[:500],
            "doc_url": row.get("doc_url", "").strip(),
            "status": row.get("status", "").strip()[:10],
            "date_publ": parse_date(row.get("date_publ", "")),
            "court_name": "",
            "judgment_name": "",
            "justice_kind_name": "",
            "category_name": "",
            "resolution_text": "",
            "import_date": timezone.now()
        }

    def _progress_monitor(self, stop_event, total_records):
        """–ú–æ–Ω—ñ—Ç–æ—Ä –ø—Ä–æ–≥—Ä–µ—Å—É –∫–æ–∂–Ω—ñ 10 —Å–µ–∫—É–Ω–¥"""
        start_time = time.time()
        
        while not stop_event.wait(10):  # –ß–µ–∫–∞—î–º–æ 10 —Å–µ–∫—É–Ω–¥ –∞–±–æ stop_event
            with self.lock:
                current_count = self.imported_count
                chunks = self.processed_chunks
            
            elapsed = time.time() - start_time
            progress_percent = (current_count / total_records * 100) if total_records > 0 else 0
            records_per_sec = current_count / elapsed if elapsed > 0 else 0
            
            self.stdout.write(
                f"‚ö° {current_count:,} –∑–∞–ø–∏—Å—ñ–≤ ({progress_percent:.1f}%) | "
                f"{records_per_sec:,.0f} –∑–∞–ø/—Å–µ–∫ | {chunks} —á–∞–Ω–∫—ñ–≤ | "
                f"{elapsed:.0f} —Å–µ–∫"
            )

    def _create_indexes_after_import(self, table_name):
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—ñ–≤ –ø—ñ—Å–ª—è —ñ–º–ø–æ—Ä—Ç—É"""
        self.stdout.write("üîß –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—ñ–≤...")
        
        with connection.cursor() as cursor:
            indexes_sql = [
                f"CREATE UNIQUE INDEX idx_{table_name}_doc_id ON {table_name} (doc_id);",
                f"CREATE INDEX idx_{table_name}_cause_num ON {table_name} USING btree (cause_num);",
                f"CREATE INDEX idx_{table_name}_court_code ON {table_name} USING btree (court_code);",
                f"CREATE INDEX idx_{table_name}_adjudication_date ON {table_name} USING btree (adjudication_date);",
                f"CREATE INDEX idx_{table_name}_case_search ON {table_name} USING btree (cause_num, court_code, adjudication_date);",
            ]
            
            for index_sql in indexes_sql:
                try:
                    cursor.execute(index_sql)
                except Exception as e:
                    logger.warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ —ñ–Ω–¥–µ–∫—Å: {e}")
            
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —Ç–∞–±–ª–∏—Ü—é –¥–æ LOGGED —Ä–µ–∂–∏–º—É —Ç–∞ –≤–∫–ª—é—á–∞—î–º–æ synchronous_commit
            cursor.execute(f"ALTER TABLE {table_name} SET LOGGED;")
            cursor.execute("SET synchronous_commit = on;")
        
        self.stdout.write("‚úÖ –Ü–Ω–¥–µ–∫—Å–∏ —Å—Ç–≤–æ—Ä–µ–Ω–æ!")